{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The first part of the assignment, IDS 2020-2021\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><b>Student Names and IDs:\n",
    "    \n",
    "    Mridul Mani Tripathi - 403587"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to convert numbers to words (for Question 2)\n",
    "!pip install inflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic stack as defined in software\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import tree, metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder, normalize\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "from p_decision_tree.DecisionTree import DecisionTree\n",
    "import scipy.cluster.hierarchy as shc\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# internals\n",
    "import inspect\n",
    "from typing import NewType\n",
    "import itertools\n",
    "import functools as func\n",
    "import operator\n",
    "\n",
    "# inflect needed to convert numbers to words (for Question 2)\n",
    "import inflect\n",
    "\n",
    "# seaborn for distribution\n",
    "import seaborn as sns\n",
    "\n",
    "# hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "# random seed\n",
    "rand_seed = 403596\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [16, 9]\n",
    "plt.rcParams['figure.dpi'] = 500\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IdsType = NewType('IdsType',str)\n",
    "\n",
    "# definitions\n",
    "\n",
    "class IdsTypes:\n",
    "    time = IdsType('Time')\n",
    "    num = IdsType('Numerical')\n",
    "    cat = IdsType('Categorical')\n",
    "    bl = IdsType('Boolean')\n",
    "\n",
    "class Column(str):  \n",
    "    def __new__(cls,name:str,idstype:IdsType):\n",
    "        obj = str.__new__(cls,name)                                    # for accessing column\n",
    "        obj.idstype = idstype\n",
    "        return obj\n",
    "    \n",
    "class CatCol(Column):\n",
    "    def __new__(cls,name:str):\n",
    "        obj = Column.__new__(cls,name, IdsTypes.cat)\n",
    "        return obj\n",
    "    \n",
    "    def convert(self,ds_col):\n",
    "        return pd.Categorical(ds_col)                                  # to convert column values  \n",
    "    \n",
    "class NumCol(Column):\n",
    "    def __new__(cls,name:str):\n",
    "        obj = Column.__new__(cls,name, IdsTypes.num)\n",
    "        return obj\n",
    "    \n",
    "    def convert(self,ds_col):\n",
    "        return pd.to_numeric(ds_col)                                  # to convert column values\n",
    "\n",
    "class BoolCol(Column):\n",
    "    def __new__(cls, name:str):\n",
    "        obj = Column.__new__(cls,name,IdsTypes.bl)\n",
    "        return obj\n",
    "\n",
    "    def convert(self, col):\n",
    "        return col.astype('bool')                                     # to convert column values           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definitions\n",
    "# for reading and loading data sets\n",
    "\n",
    "class Dataset:\n",
    "    file_name = 'dataset.csv'\n",
    "    sampled_file_name = 'sampled_data.csv'\n",
    "    __original_ds__ = None\n",
    "    __sampled_ds__ = None\n",
    "    \n",
    "    def load_original(force_reload = False):\n",
    "        if force_reload or Dataset.__original_ds__ is None:\n",
    "            Dataset.__original_ds__ = Dataset.apply_categories(pd.read_csv(Dataset.file_name))\n",
    "        return Dataset.__original_ds__.copy()\n",
    "    \n",
    "    def load_sampled(force_reload = False):        \n",
    "        if force_reload or Dataset.__sampled_ds__ is None:\n",
    "            Dataset.__sampled_ds__ = Dataset.apply_categories(pd.read_csv(Dataset.sampled_file_name))\n",
    "        return Dataset.__sampled_ds__.copy()\n",
    "    \n",
    "    def apply_categories(df):\n",
    "        for w in Dataset.Cols.as_set():\n",
    "                df[w] = w.convert(df[w])\n",
    "        return df.set_index(Dataset.Cols.id)\n",
    "    \n",
    "    class Cols:\n",
    "        id = NumCol('ID')\n",
    "        surfaceR = NumCol('SurfaceR')\n",
    "        numberR = NumCol('NumberR')\n",
    "        typeR = CatCol('TypeR')\n",
    "        vegetationR = NumCol('VegetationR')\n",
    "        surroundings1 = CatCol('Surroundings1')\n",
    "        surroundings2 = CatCol('Surroundings2')\n",
    "        surroundings3 = CatCol('Surroundings3')\n",
    "        useR = NumCol('UseR')\n",
    "        fishingR = NumCol('FishingR')\n",
    "        acessR = NumCol('AcessR')\n",
    "        roadDistanceR = NumCol('RoadDistanceR')\n",
    "        buildingR = NumCol('BuildingR')\n",
    "        pollutionR = NumCol('PollutionR')\n",
    "        shoreR = CatCol('ShoreR')\n",
    "        green_frogs = BoolCol('Green frogs')\n",
    "        brown_frogs = BoolCol('Brown frogs')\n",
    "        common_toad = BoolCol('Common toad')\n",
    "        fire_toad = BoolCol('Fire-bellied toad')\n",
    "        tree_frogs = BoolCol('Tree frog')\n",
    "        common_newt = BoolCol('Common newt')\n",
    "        great_newt = BoolCol('Great crested newt')\n",
    "        __intset__ = None\n",
    "        __transdict__ = None\n",
    "        \n",
    "        def _intset_():\n",
    "            if Dataset.Cols.__intset__ is None:\n",
    "                Dataset.Cols.__intset__ = frozenset({val for _,val in inspect.getmembers(Dataset.Cols, lambda attr:isinstance(attr,Column))})\n",
    "            return Dataset.Cols.__intset__\n",
    "            \n",
    "            \n",
    "        def _transdict_():\n",
    "            if Dataset.Cols.__transdict__ is None:\n",
    "                Dataset.Cols.__transdict__ = {v:v for v in Dataset.Cols.as_set()}\n",
    "            return Dataset.Cols.__transdict__\n",
    "            \n",
    "        def as_set():\n",
    "            return {v for v in Dataset.Cols._intset_()}\n",
    "        \n",
    "        def as_list():\n",
    "            return [v for v in Dataset.Cols._intset_()]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definitions\n",
    "\n",
    "class TTSGroup:\n",
    "    class TTS:\n",
    "        def __init__(self,train,test):\n",
    "            self.train = train\n",
    "            self.test = test\n",
    "\n",
    "    def __init__(self, X_train, X_test, y_train, y_test):\n",
    "        self.X = TTSGroup.TTS(X_train, X_test)\n",
    "        self.y = TTSGroup.TTS(y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we are interested in the values of the individual frogs and not the overall combination of frogs, we created a class which extracts individual accuracy scores automaticly and implements comparison operators using the sum of all individual scores.\n",
    "\n",
    "class MultiClassAccs:\n",
    "\n",
    "    def __init__(self, true_vals, predicted_vals):\n",
    "        self.individual_accs = {feature:metrics.accuracy_score(vals, [a[i] for a in predicted_vals]) for i, (feature,vals) in enumerate(true_vals.iteritems())}\n",
    "        self.total_combination_acc = metrics.accuracy_score(true_vals, predicted_vals)\n",
    "\n",
    "    def total(self):\n",
    "        return sum(self.individual_accs.values())\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        if isinstance(other, int):\n",
    "            other_val = other\n",
    "        else:\n",
    "            other_val = other.total()\n",
    "        return self.total() < other_val\n",
    "\n",
    "    def __le__(self,other):\n",
    "        if isinstance(other, int):\n",
    "            other_val = other\n",
    "        else:\n",
    "            other_val = other.total()\n",
    "        return self.total() <= other_val\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, int):\n",
    "            other_val = other\n",
    "        else:\n",
    "            other_val = other.total()\n",
    "        return self.total() == other_val\n",
    "\n",
    "    def __ne__(self,other):\n",
    "        if isinstance(other, int):\n",
    "            other_val = other\n",
    "        else:\n",
    "            other_val = other.total()\n",
    "        return self.total() != other_val\n",
    "\n",
    "    def __gt__(self,other):\n",
    "        if isinstance(other, int):\n",
    "            other_val = other\n",
    "        else:\n",
    "            other_val = other.total()\n",
    "        return self.total() > other_val\n",
    "    \n",
    "    def __ge__(self,other):\n",
    "        if isinstance(other, int):\n",
    "            other_val = other\n",
    "        else:\n",
    "            other_val = other.total()\n",
    "        return self.total() >= other_val\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Combined: {}, {}\".format(self.total_combination_acc, str(self.individual_accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_ = Dataset.load_original(True)\n",
    "#_ = Dataset.load_sampled(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing of the Dataset (5 points)\n",
    " Carry out the following preprocessing steps before starting the analysis:\n",
    " - Select 90% of dataset provided for this assignment by random sampling.\n",
    "     - Use one of the group member's student numbers as a seed.\n",
    "     - Rename the new generated dataset (which contains 90% of the data) to \"sampled_data\".\n",
    " - <font color='red'>Important!</font>  Export your *sampled_data* dataset and submit it with your assignment solution.\n",
    " - If it is not otherwise mentioned, you should always use your below created *sampled_data* as input for the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_ds = Dataset.load_original()                                  # loading original dataset \n",
    "sampled_ds = orig_ds.sample(frac=0.9, random_state=rand_seed)      # random sampling of data set\n",
    "sampled_ds.to_csv(Dataset.sampled_file_name, index=True)           # Converting and saving the dataframe to .csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 - Insights into the Data (15 points):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   (a)  Generate a dataset by removing those rows of the sampled_data dataset for which the value of \"SurfaceR\" is equal or bigger than 50000. Let's call this data set \"new_sampled_data\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q1_a:\n",
    "    \n",
    "    def run(self, base):\n",
    "        self.new_sampled_data = base[base[Dataset.Cols.surfaceR]<50000]       # Removing rows of sampled_data for which values\n",
    "        return self                                                          # of 'SurfaceR' is equal or greater than 50000 \n",
    "    \n",
    "q1_a = Q1_a().run(Dataset.load_sampled())\n",
    "q1_a.new_sampled_data.describe()\n",
    "\n",
    "# export data in case it is required by automated testing\n",
    "new_sampled_data = q1_a.new_sampled_data\n",
    "new_sampled_data.to_csv(\"new_sampled_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   (b)  Use a boxplot to find and remove the outliers from \"SurfaceR\". Note that based on the boxplot the values greater than the upper-whisker and lower than the lower-whisker are considered as outliers. Let's call the dataset after removing the outliers \"cleaned_data\". Now you should  have three datasets (sampled_data, new_sampled_data, and cleaned_data). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q1_b:\n",
    "    \n",
    "    def run(self, df, df_name):\n",
    "        Q1 = df['SurfaceR'].quantile(0.25)\n",
    "        Q3 = df['SurfaceR'].quantile(0.75)\n",
    "        IQR = Q3 - Q1                                                        # IQR is interquartile range\n",
    "        \n",
    "        # identifying outliers and filtering them\n",
    "        filter = (df['SurfaceR'] >= Q1 - 1.5 * IQR) & (df['SurfaceR'] <= Q3 + 1.5 *IQR)\n",
    "        self.cleaned_data = df.loc[filter]\n",
    "        cd = 'cleaned_data'                                                  # data set without outliers \n",
    "        sax = self.cleaned_data.boxplot('SurfaceR', return_type='axes')      # box plot for cleaned_data\n",
    "        sax.set_xlabel(cd)\n",
    "        \n",
    "        p, (ax1, ax2) = plt.subplots(1,2,sharey=True)\n",
    "        p.suptitle(\"Side-by-side comparison of new_sampled_data and cleaned_data\")\n",
    "        df.boxplot('SurfaceR', ax=ax1)                                       # box plot for comparision \n",
    "        ax1.set_xlabel(df_name)\n",
    "        self.cleaned_data.boxplot('SurfaceR',ax=ax2)\n",
    "        ax2.set_xlabel(cd)\n",
    "        p.tight_layout()\n",
    "        return self\n",
    "        \n",
    "q1_b = Q1_b().run(new_sampled_data, \"new_sampled_data\")\n",
    "\n",
    "# export data in case it is required by automated testing\n",
    "cleaned_data = q1_b.cleaned_data \n",
    "cleaned_data.to_csv(\"cleaned_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   (c) Compare basic statistical features of \"SurfaceR\" (median, mean, and mode, standard deviation, variance) in the new_sampled_data and cleaned_data datasets.    Interpret the differences for these statistical values between the cleaned_data and new_sampled_data datasets. Explain why the statistics of these two datasets are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical features of \"SurfaceR\" in cleaned_data and new_sampled_data\n",
    "\n",
    "class Q1_c:\n",
    "    \n",
    "    target_col = Dataset.Cols.surfaceR\n",
    "\n",
    "    def run(self,**dataframes):\n",
    "        self.compared = pd.concat({name:self.get_stat_features(df) for name, df in dataframes.items()}, axis=1)\n",
    "        return self\n",
    "\n",
    "    def get_stat_features(self, df):\n",
    "        target = df[Q1_c.target_col]\n",
    "        md = stats.mode(target)\n",
    "        add = pd.Series([np.median(target),np.mean(target), \n",
    "                         \"{} (#{})\".format(md.mode[0], md.count[0]),\n",
    "                         np.std(target), np.var(target) ], \n",
    "                        index=['median', 'mean', 'mode', 'standard deviation','variance'  ])\n",
    "        \n",
    "        return add\n",
    "\n",
    "# use exported data in case it is required by automated testing\n",
    "q1_c = Q1_c().run(new_sampled_data = new_sampled_data, cleaned_data = cleaned_data)\n",
    "q1_c.compared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: We can observe that the there is a huge influence of the outliers in the data on statistical features, if we observe the box plots of both the datasets, we see that some values of Surface R in new_sample_data reaches as far as 40000, which is well beyond the Interquartile range (IQR) and outside the upper whisker. These data samples will skew our decision boundary, even though the actual clustering is not in that direction. \n",
    "This further results in very high variance and standard deviaction, as outliers increase the value of | x - x_mean |^2, also the influence of these outliers also increase the mean, and median where as mode is unalterd since it is well within the IQR "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Visualization (10 points)\n",
    "(d) Visualize mean and median of \"SurfaceR\" in the cleaned dataset. Specify the \"Surroundings3\" values for which the mean and median of \"SurfaceR\" is maximal and for which it is minimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation of cleaned_data\n",
    "\n",
    "class Q1_d:\n",
    "    \n",
    "    target_col = Dataset.Cols.surfaceR\n",
    "    values_col = Dataset.Cols.surroundings3\n",
    "\n",
    "\n",
    "    def run(self, cleaned_df):\n",
    "        self.scatter(cleaned_df)\n",
    "        self.SurroundingsMetric(cleaned_df)\n",
    "        self.bars(cleaned_df)\n",
    "        return self\n",
    "\n",
    "    def scatter(self, df):                                                                  # visualising mean and median\n",
    "        plt.plot(df[Q1_d.target_col], 'o', label=Q1_d.target_col, color=\"#F5B08B\")         # using scatter plot\n",
    "        plt.title(\"Mean and median of {}\".format(Q1_d.target_col))\n",
    "        plt.axhline(df[Q1_d.target_col].mean(), label=\"mean\", color=\"red\")\n",
    "        plt.axhline(df[Q1_d.target_col].median(), label=\"median\", color=\"blue\")\n",
    "        plt.legend()\n",
    "\n",
    "    def bars(self, df):\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
    "        fig.suptitle = \"Analysis of {} values vs. {}\".format(Q1_d.values_col, Q1_d.target_col)\n",
    "        ax1.title.set_text(\"Mean\")                                                         # visualising mean\n",
    "        sns.barplot(x=Q1_d.values_col, y =Q1_d.target_col, data = cleaned_data, estimator = np.mean, ax=ax1)\n",
    "        ax2.title.set_text(\"Median\")                                                       # visualising median\n",
    "        sns.barplot(x=Q1_d.values_col, y =Q1_d.target_col, data = cleaned_data, estimator = np.median, ax=ax2)\n",
    "    \n",
    "    def SurroundingsMetric(self, cleaned_df):                                              # for tabular representation\n",
    "        df = cleaned_df.loc[:,[Dataset.Cols.surfaceR, Dataset.Cols.surroundings3]]\n",
    "        mean = df.groupby(Dataset.Cols.surroundings3).mean()\n",
    "        med = df.groupby(Dataset.Cols.surroundings3).median()\n",
    "        idxInd = lambda x: x[0]\n",
    "        metric_col = 'metric'\n",
    "        mn = 'mean'\n",
    "        md = 'median'\n",
    "        ex_col = 'extreme'\n",
    "        mini = 'min'\n",
    "        maxi = 'max'\n",
    "        self.result_df = pd.DataFrame({Dataset.Cols.surroundings3:[idxInd(ex) for ex in (mean.idxmin(), mean.idxmax(), med.idxmin(), med.idxmax())],metric_col:[mn, mn, md,md], ex_col:[mini,maxi,mini,maxi]})\n",
    "\n",
    "# use exported data in case it is required by automated testing\n",
    "q1_d= Q1_d().run(cleaned_data)\n",
    "q1_d.result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: The mean and median of `SurfaceR` are minimal for value ' d ' of `Surroundings3`. The maximum value for both metrics is ' i '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   (e) Plot the distribution of \"SurfaceR\" in the new_sampled_data and cleaned_data datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q1_e:\n",
    "    \n",
    "    target_col = Dataset.Cols.surfaceR\n",
    "    def run(self, new_sampled_df, cleaned_df):\n",
    "        fig, axes = plt.subplots(2,1, sharex=True, sharey=True)\n",
    "        sns.histplot(new_sampled_df[Q1_e.target_col], color=\"blue\", kde=True, ax=axes[0])    # For distribution of SurfaceR \n",
    "        axes[0].title.set_text(\"New sampled data\")                                          # in new_sampled_data\n",
    "        \n",
    "        sns.histplot(cleaned_df[Q1_e.target_col], color=\"red\", kde=True, ax=axes[1])         # For distribution of SurfaceR \n",
    "        axes[1].title.set_text(\"Cleaned data\")                                              # in cleaned_data\n",
    "        fig.tight_layout()                                                                 \n",
    "        plt.show()\n",
    "        return self\n",
    "    \n",
    "q1_e = Q1_e().run(new_sampled_data, cleaned_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   (f) Explore the distribution of \"SurfaceR\" and \"AcessR\" together in the new_sampled_data and cleaned_data datasets. Specify the ranges of \"SurfaceR\" and \"AcessR\" for which the frequency of the data is the highest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of \"SurfaceR\" and \"AcessR\" together\n",
    "\n",
    "class Q1_f:\n",
    "    \n",
    "    coi = [Dataset.Cols.surfaceR, Dataset.Cols.acessR]\n",
    "    def run(self, **dataframes):\n",
    "        self.bars(dataframes)\n",
    "        self.dists(dataframes)\n",
    "        return self\n",
    "\n",
    "    def dists(self, dataframes):                                       # distribution plot\n",
    "        p, axs = plt.subplots(1,2,  sharey=True)\n",
    "        for i, (name, df) in enumerate(dataframes.items()):\n",
    "            sns.kdeplot(x=Q1_f.coi[0], y = Q1_f.coi[1], data = df, ax = axs[i])\n",
    "            axs[i].set_title(name)\n",
    "        p.tight_layout\n",
    "\n",
    "\n",
    "    def bars(self, dataframes):                                        # bar plot \n",
    "        for _, (x_val, y_val) in enumerate(itertools.permutations(Q1_f.coi,2)):\n",
    "            fig, axes = plt.subplots(1,len(dataframes), figsize=(16,9), sharex=True, sharey=True)\n",
    "            fig.suptitle(y_val + \" count by \" + x_val + \" value\")\n",
    "            for i, (name, df) in enumerate(dataframes.items()):\n",
    "                Q1_f.process_df(df, axes[i], x_val,y_val)\n",
    "                axes[i].set_title(name)\n",
    "            fig.tight_layout()\n",
    "    \n",
    "    def process_df(dfr, ax, x_val, y_val):\n",
    "        df = dfr.loc[:, (x_val,y_val)]\n",
    "        data = df.groupby(x_val).agg('count')\n",
    "        ax.hist(data)\n",
    "        ax.set_xlabel(x_val)\n",
    "        ax.set_ylabel(y_val + \" count\")\n",
    "            \n",
    "q1_f = Q1_f().run(new_sampled_data = new_sampled_data, cleaned_data = cleaned_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "For both datasets, the highest frequency happens when AcessR = 100 and SurfaceR = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 - Decision Trees (15 points):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   (a) Add a categorical column \"number_frogs\" to the new_sampled_data which indicate the number of different frogs in each region (row). For example, if in a row we have:\n",
    "       - \"Green frogs\" = 1, \"Brown frogs\" = 1, \"Common toad\" = 0, \"Fire-bellied toad\" = 0, \"Tree frog\" = 0, \"Common newt\" = 0, and \"Great crested newt\" = 0, then \"number_frogs\" = 'two'.\n",
    "       - \"Green frogs\" = 1, \"Brown frogs\" = 1, \"Common toad\" = 0, \"Fire-bellied toad\" = 0, \"Tree frog\" = 1, \"Common newt\" = 1, and \"Great crested newt\" = 0, then \"number_frogs\" = 'four'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q2:\n",
    "    \n",
    "    class Cols:\n",
    "        frogc = CatCol('number_frogs')                        # new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q2_a:\n",
    "    \n",
    "    rows = frozenset({Dataset.Cols.green_frogs,               # columns for frog types\n",
    "                      Dataset.Cols.brown_frogs,\n",
    "                      Dataset.Cols.common_toad,\n",
    "                      Dataset.Cols.fire_toad,\n",
    "                      Dataset.Cols.tree_frogs,\n",
    "                      Dataset.Cols.common_newt,\n",
    "                      Dataset.Cols.great_newt})\n",
    "    \n",
    "    def run(self, df):\n",
    "        p = inflect.engine()                                  # converting number to words\n",
    "        pdct = {0:'zero', 1:'one', 2:'two', 3:'three', 4:'four', 5:'five', 6:'six', 7:'seven'}\n",
    "        df[Q2.Cols.frogc] = Q2.Cols.frogc.convert(df[Q2_a.rows].sum(axis=1).transform(lambda x: p.number_to_words(x)))\n",
    "        self.categorized_data = df\n",
    "        return self\n",
    "\n",
    "q2_a = Q2_a().run(new_sampled_data.copy(deep=True))\n",
    "q2_a.categorized_data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   (b) In the new dataset (created in Section 'a'), consider \"TypeR\", \"VegetationR\", \"Surroundings1\", \"Surroundings2\", \"Surroundings3\" as    the descriptive features and \"number_frogs\" as the target feature. Generate two decision trees. Let's call them \"tree1\" and \"tree2\". In tree1 set the minimum number of samples for splitting to 15 and in tree2 set the minimum number of samples for splitting to 1. Create both decision trees based on entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q2_b:\n",
    "    \n",
    "    descriptive_features= [Dataset.Cols.typeR,                            # descriptive features\n",
    "                                    Dataset.Cols.vegetationR,\n",
    "                                    Dataset.Cols.surroundings1,\n",
    "                                    Dataset.Cols.surroundings2,\n",
    "                                    Dataset.Cols.surroundings3]\n",
    "    target_feature = Q2.Cols.frogc                                        # target feature\n",
    "    \n",
    "    def run(self, df):\n",
    "        self.tree1= Q2_b.descisionTree(df, 15)                            # minimum number of samples for splitting = 15\n",
    "        self.tree2= Q2_b.descisionTree(df, 1)                             # minimum number of samples for splitting = 1\n",
    "        return self\n",
    "    \n",
    "    def descisionTree(df, min_split):\n",
    "        descriptive_data = df[Q2_b.descriptive_features].values.tolist()\n",
    "        descriptive_data = [[str(x) for x in a] for a in descriptive_data]\n",
    "        descriptive_features = df[Q2_b.descriptive_features].columns.tolist()\n",
    "        target_data = df[Q2_b.target_feature].values.tolist()\n",
    "        tree = DecisionTree(descriptive_data, descriptive_features, target_data, \"entropy\")\n",
    "        tree.id3(0,min_split)\n",
    "\n",
    "        return tree\n",
    "    \n",
    "q2_b = Q2_b().run(q2_a.categorized_data)\n",
    "tree1 = q2_b.tree1\n",
    "tree2 = q2_b.tree2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = tree1.print_visualTree(render=True)                                   # printing decision tree 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = tree2.print_visualTree(render=True)                                   # printing decision tree 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing entropy for decision trees\n",
    "\n",
    "print(\"Entopy tree1: {}\".format(tree1.entropy))\n",
    "print(\"Entopy tree2: {}\".format(tree2.entropy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   (c) Consider tree1. What is the best attribute (based on entropy) for splitting the tree in the second round of ID3 regarding the value of the attribute chosen in the first round of ID3?    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: In the first round of ID3 the maximun information gain (least entropy)  can be found in TypeR feature, thus it is used for splitting. In the second round \"surrounding3\" would have the least entropy (maximun information gain), thus surrounding3 feature will be used for splitting the tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   (d) Compare tree1 and tree2 in terms of the possibility of overfitting and the complexity of the decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: Since the minimum samples for splitting is set to 1 in Tree2, there is a overfitting in Tree2 as all the samples are splitted to it's class individualy (even per sample), the generality is lost, thus leading to complex decision tree. But in Tree1 we can observe a good fit as we set at least 15 samples needed for next split, thus maintaining generality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 - Regression (14 points):\n",
    "\n",
    "For this question (Q3), create and use a restricted dataset by removing the columns \"ID\", \"NumberR\", \"Surrounding1\", \"Surrounding2\", \"Surrounding3\", \"Common toad\", \"Fire-bellied toad\", \"Tree frog\", \"Common newt\", \"Great crested newt\" from the sampled_data.\n",
    "\n",
    "In this question, we consider \"Green frogs\" and \"Brown frogs\" to be potential target features, while all other features are potential descriptive features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q3:\n",
    "    \n",
    "    target_features=[Dataset.Cols.green_frogs, Dataset.Cols.brown_frogs]             # target feature\n",
    "    \n",
    "    excluded_columns=[Dataset.Cols.numberR, Dataset.Cols.surroundings1,              # for restricted data set\n",
    "                      Dataset.Cols.surroundings2, Dataset.Cols.surroundings3, \n",
    "                      Dataset.Cols.common_toad, Dataset.Cols.fire_toad, \n",
    "                      Dataset.Cols.tree_frogs, Dataset.Cols.common_newt, Dataset.Cols.great_newt]\n",
    "    \n",
    "    def __init__(self, base_df):                                                     \n",
    "        self.descriptive_features = [c for c in Dataset.Cols.as_list() if c not in Q3.excluded_columns and c not in Q3.target_features and c is not Dataset.Cols.id]\n",
    "        self.df = base_df.drop(columns= self.excluded_columns)                       # creating restricted data set\n",
    "\n",
    "\n",
    "q3 = Q3(Dataset.load_sampled())\n",
    "print(\"Descriptive features: {}\".format(q3.descriptive_features))\n",
    "print(\"Target features: {}\".format(q3.target_features))\n",
    "q3.df.to_csv(\"restricted_data.csv\")\n",
    "q3.df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   (a) Which features are suitable as input for logistic regression? Which would need to be modified first? Explain your answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: Logistic regression is able to handle continuous and categorical features(after modifications). The suitable input for LR is continuous variable. Since the descriptive feature \"TypeR\" and \"ShoreR\" here uses categorical data we would have to modify these using one hot encoding/use dummies.\n",
    "\n",
    "           Features suitable for input: SurfaceR, VegetationR, UseR, FishingR, AcessR, RoadDistanceR, BuildingR, PollutionR\n",
    "\n",
    "           Features that need modification: TypeR, ShoreR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   (b) Implement and briefly motivate an adequate modification. Print the resulting data set limited to the first two data rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding (creating dummy columns) for categorical descriptive features\n",
    "\n",
    "class Q3_b:\n",
    "\n",
    "    def run(self, base_df):\n",
    "        self.encoded_df = pd.get_dummies(base_df)\n",
    "        self.encoded_inputs = [v for v in self.encoded_df.columns.values if v not in Q3.target_features]\n",
    "        return self\n",
    "        \n",
    "q3_b = Q3_b().run(q3.df)\n",
    "q3_b.encoded_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: As explained in the previous cell, \"TypeR\" has categorical features, using the pandas.get_dummies(...), we can get one hot encoding with 0/1 values as feature values, which can now we use for logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) We want to predict the presence of green frogs and brown frogs in the habitat, using a distinct logistic regression classifier for each frog type. \n",
    "\n",
    "Consider the set of features available in this question's unmodified data set (that is before Q3b). To get an overview of the data, choose and present some basic visualization as discussed in the lectures (e.g.  scatter matrix, scatter plots, charts, etc.). Based on this visualization, for each frog type choose the 4 most promising descriptive features to predict the presence of that frog type in the habitat. \n",
    "\n",
    "Explain your strategy and choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Various plots for visualidation help in choosing promising descriptive features\n",
    "\n",
    "class Q3_c:\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def run_pairplots(self):\n",
    "        for target in Q3.target_features:\n",
    "            defaults = {\"hue\":target, \"palette\":{1:target.split(' ')[0], 0:'grey'}, \"corner\":True}\n",
    "            sns.pairplot(self.df,vars=q3.descriptive_features, kind='hist', **defaults)\n",
    "            sns.pairplot(self.df,vars=[c for c in q3.descriptive_features if pd.api.types.is_numeric_dtype(self.df[c].dtypes)], kind='kde', **defaults)\n",
    "        \n",
    "    \n",
    "    def run_smalplots(self):\n",
    "        feats = [Dataset.Cols.typeR, Dataset.Cols.fishingR, Dataset.Cols.surfaceR, Dataset.Cols.pollutionR]\n",
    "        p, ax = plt.subplots(len(Q3.target_features), len(feats), figsize=(16,9))\n",
    "        for i, target in enumerate(Q3.target_features):\n",
    "            for n, feature in enumerate(feats):\n",
    "                sns.barplot(x=feature, y=target, data=self.df, ax=ax[i][n])\n",
    "        p.tight_layout()\n",
    "    \n",
    "q3_c = Q3_c(q3.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q3_c.run_pairplots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q3_c.run_smalplots()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:  From the visualization plots shown above, we should consider the features, that have high corelation with the target features. If we see the \"Pairplots\" we can observe that for both green and brown frogs there is a high corelation with:\n",
    "1. Pollution \n",
    "2. TypeR \n",
    "3. FishingR\n",
    "4. AcessR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   (d) For both frog types, train a logistic regression classifier to predict the presence of that frog type in the habitat. Use the descriptive features as chosen in Q3c. Apply the modification from Q3b if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training a logistic regression classifier to predict the presence of green frog and brown frog in the habitat\n",
    "\n",
    "class Q3_d:\n",
    "\n",
    "    def __init__(self, targets, descriptive, df):\n",
    "        self.targets = targets\n",
    "        self.descriptive = descriptive\n",
    "        self.df = df\n",
    "        self.target_names = list()\n",
    "        self.classifiers = list()\n",
    "        self.predictions = list()\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        df = self.df\n",
    "        for target in self.targets:\n",
    "            classifier = LogisticRegression(solver = 'liblinear', multi_class='ovr', random_state=rand_seed)\n",
    "            classifier.fit(df[self.descriptive], df[target])\n",
    "            prediction = classifier.predict(df[self.descriptive])\n",
    "            self.target_names.append(target)\n",
    "            self.classifiers.append(classifier)\n",
    "            self.predictions.append(prediction)\n",
    "        return self\n",
    "\n",
    "# descriptive features selected from Q3c and created dummy columns for categorical descriptive features    \n",
    "    \n",
    "q3_d = Q3_d(Q3.target_features, [f for f in q3_b.encoded_inputs if f in {Dataset.Cols.acessR, Dataset.Cols.fishingR, Dataset.Cols.pollutionR} or f.startswith(Dataset.Cols.typeR)], q3_b.encoded_df).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({target:[q3_d.classifiers[i].coef_,  q3_d.classifiers[i].intercept_,q3_d.classifiers[i].score(q3_d.df[q3_d.descriptive], q3_d.df[target])] for i, target in enumerate(q3_d.targets)}, index=['Coefficient', 'Intercept', 'Score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   (e) For each of the two trained classifiers compute and print the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing confusion matrices\n",
    "\n",
    "for i, target in enumerate(q3_d.targets):\n",
    "    print('Confusion Matrix for {}:\\n{}'.format(target, metrics.confusion_matrix(q3_d.df[target], q3_d.predictions[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(f) Based on the information computed in Q3 so far, interpret and evaluate the two models and compare them. Why are they similar/different? Would you recommend the models and why (not)? How do you think the applied methods could be improved to get better results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing models\n",
    "\n",
    "for i, target in enumerate(q3_d.targets):\n",
    "    df = q3_d.df\n",
    "    print('##################\\n{}'.format(target))\n",
    "    print('Accuracy: ', q3_d.classifiers[i].score(df[q3_d.descriptive], df[target]))\n",
    "    print('Classification report:')\n",
    "    print(metrics.classification_report(df[target], q3_d.predictions[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: For the first case (Green Frogs) we have score of 74.11% and if we observe the confusion matrix, we find a similarity that the True Positives high for both cases. But in contrast True Negatives are high for first case, but in the second case although score is 77.64% there are many False Positives are far less True Negetives. So these attributes Predict \"Green Frogs\" in habitat better than of \"Brown Frogs\". So these attribues are recommended for Green Frogs. But we need to ADD other features to predict Brown frogs better and more trainding data could be useful as well. From the classification report we can see that presence of both Green and Brown frogs in the habitat are well represented, but absence of Brown frogs in habitat is poorly represented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 - Support Vector Machines (8 points):\n",
    "\n",
    "For this question (Q4), restrict your data set to the same features as in Q3. Similar to Q3, we want to train two distinct classifiers predicting the presence of green frogs and brown frogs in the habitat. \n",
    "\n",
    "In this question, we will use SVMs instead of logistic regression. In the following, consider *Green frogs* and *Brown frogs* to be potential target features, while all other features are potential descriptive features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting descriptive features and target features\n",
    "\n",
    "class Q4:\n",
    "    \n",
    "    target_features= Q3.target_features\n",
    "    excluded_columns=Q3.excluded_columns\n",
    "    def __init__(self, base_df):\n",
    "        self.descriptive_features = [c for c in Dataset.Cols.as_list() if c not in Q4.excluded_columns and c not in Q4.target_features and c is not Dataset.Cols.id]\n",
    "        self.df = base_df.drop(columns= self.excluded_columns)\n",
    "\n",
    "q4 = Q4(Dataset.load_sampled())\n",
    "q4.df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   (a) Which of the potential descriptive features are suitable as an input for SVMs and which need to be modified first? Modify the data as needed and provide a brief explanation. Print the first two data rows of the modified data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding (creating dummy columns) for categorical descriptive features\n",
    "\n",
    "class Q4_a:\n",
    "\n",
    "    def run(self, base_df):\n",
    "        self.encoded_df = pd.get_dummies(base_df)\n",
    "        self.encoded_inputs = [v for v in self.encoded_df.columns.values if v not in Q4.target_features]\n",
    "        return self\n",
    "\n",
    "q4_a = Q4_a().run(q4.df)\n",
    "q4_a.encoded_df.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:  The input for SVM should be also continuous descriptive features, just like in Regression. So we copy the same dataframes from Regression, where categorical features like \"ShoreR\" and \"TypeR\" are converted into continuous data in the sense of one hot encoding.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   (b) For each frog type, consider the same set of 4 descriptive features as chosen in Q3 c). Generate for both target features a training and test set based on all data rows (for example, consider the sampling strategies as explained in the lecture) of the restricted data set. Briefly explain and motivate the choice of the sampling strategy as well as the size of the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data into training and test set,\n",
    "\n",
    "class Q4_b:\n",
    "    defaults = {\"test_size\":0.33, \"shuffle\":True, \"random_state\":rand_seed}\n",
    "\n",
    "    def run(self, df, descriptives, green_target, brown_target):\n",
    "        self.green_set = TTSGroup(*train_test_split(df[descriptives], df[green_target], **Q4_b.defaults))\n",
    "        self.brown_set = TTSGroup(*train_test_split(df[descriptives], df[brown_target], **Q4_b.defaults))\n",
    "        return self\n",
    "\n",
    "q4_b = Q4_b().run(q4_a.encoded_df, q4_a.encoded_inputs, Dataset.Cols.green_frogs, Dataset.Cols.brown_frogs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: Here we have split the data into training and test set, where 1/3 of the shuffled data is seperated and used for testing while 2/3 of data split will be used for Training. In the lecture it was advised to use the suffled data since data could be biased when it is stored in dataframe. This removes any effect that is resulted from the way of storing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   (c) Use the training set to train 4 different SVMs (2 per frog type) with different parameter combinations. Use at least two distinct values for the parameters *kernel* and *C*.\n",
    "\n",
    "*Hint: depending on the size of the training data and chosen parameters, training the SVMs may take some time.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q4_c:\n",
    "\n",
    "    defaults = {\"gamma\":'auto', \"random_state\":rand_seed}\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        self.greens = list()\n",
    "        self.browns = list()\n",
    "\n",
    "    def run(self, green_set, brown_set):\n",
    "        self.make_svms()\n",
    "        Q4_c.train_svms_for_target(self.greens, green_set)                 # for green frog  \n",
    "        Q4_c.train_svms_for_target(self.browns, brown_set)                 # for brown frog\n",
    "        return self\n",
    "        \n",
    "    def train_svms_for_target(svms, tts):\n",
    "        for svm in svms:\n",
    "            svm.fit(tts.X.train, tts.y.train)                              # training SVM\n",
    "        \n",
    "    def make_svms(self):                                                   # four different SVMs (2 per frog type)\n",
    "        self.greens.append(SVC(C=1,  kernel='sigmoid', **Q4_c.defaults))\n",
    "        self.greens.append(SVC(C=10, kernel='linear',  **Q4_c.defaults))\n",
    "        self.browns.append(SVC(C=5,  kernel='linear',  **Q4_c.defaults))\n",
    "        self.browns.append(SVC(C=10, kernel='rbf',     **Q4_c.defaults))\n",
    "\n",
    "q4_c = Q4_c().run(q4_b.green_set, q4_b.brown_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   (d) Compute and print the mean accuracy and the classification report of the trained SVMs with respect to the test set (see instruction for examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing mean accuracy and classification report of the trained SVMs\n",
    "\n",
    "class Q4_d:\n",
    "\n",
    "    def __init__(self, green_set, brown_set):\n",
    "        self.green_set = green_set\n",
    "        self.brown_set = brown_set\n",
    "\n",
    "    def run(self, greens, browns):\n",
    "        Q4_d.run_single(\"Green Frogs\", self.green_set, greens)\n",
    "        Q4_d.run_single(\"Brown Frogs\", self.brown_set, browns)\n",
    "        return self\n",
    "\n",
    "    def run_single(name, data, svms ):\n",
    "        print('#####################')\n",
    "        print(name)\n",
    "        for i, svm in enumerate(svms):\n",
    "            print('------------')\n",
    "            print('----Model {}:'.format(i+1))\n",
    "            pred = svm.predict(data.X.test)\n",
    "            print('\\tAcc: {}'.format(svm.score(data.X.test, data.y.test)))\n",
    "            print('Classification report:')\n",
    "            print(metrics.classification_report(data.y.test, pred))\n",
    "\n",
    "q4_d = Q4_d(q4_b.green_set, q4_b.brown_set).run(q4_c.greens, q4_c.browns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   (e) Based on the information computed in Q4 so far, interpret and evaluate the 4 SVMs and compare them. Why are they similar/different? Would you recommend using these SVMs and why (not)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: From the results presented above (in output), we can observe that the value of accuracy is around 80% for Green Frogs (also linear, rbf kernals record better metrics than poly fit) but its very less for Brown frogs. Further the other metric parameters such as recall, accuracy, f1 score etc.. are better for Green frogs and especially worse to see if Brown frog is absent in the habitat, and the percentage of presence of both green and brown frogs in habitat is good.\n",
    "But if we compare the classification_report for Regression and SVM, we see that the presence of frogs in both habitats are predicted well by both SVM and Regression, but the absence of Brown frogs are although bad, better represented in Regression.\n",
    "\n",
    "\n",
    "So for this particular case, we prefer Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5 - Neural Networks (15 points)\n",
    "In this question consider the sampled_data, which is the dataset that you have created in the *Preprocessing of Dataset* section. The target features are the *different frogs*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   (a) What are the possible inputs of your network?\n",
    "   \n",
    "     - Give the number of possible values of the different categorical inputs.\n",
    "     - Give the number of possible input patterns for the categorical data.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q5_a:\n",
    "\n",
    "    def run(self, df:pd.DataFrame):\n",
    "        categorical_columns = [c for c in Dataset.Cols.as_set() if type(c) is CatCol ]     # get all categorical columns\n",
    "        \n",
    "        possible_data = lambda col: ', '.join(sorted(df[col].unique()))                    # lambdas to get a string of all \n",
    "                                                                                          # unique values and their count\n",
    "            \n",
    "        unique_count = lambda col: len(df[col].unique())                                   # column names which are used\n",
    "        feat_name_col, val_count_col ='feature', 'unique value count'                     # multiple times as variables \n",
    "                                                                                         # to avoid typo-confusion \n",
    "                                                                                             \n",
    "\n",
    "        features = pd.DataFrame([[col, unique_count(col), possible_data(col)] for col in categorical_columns], columns=[feat_name_col, val_count_col , 'possible values'])\n",
    "        features.index = features[feat_name_col]                                           # actually build the dataframe \n",
    "        features.drop(feat_name_col, axis=1, inplace = True)                              # containing the unique values\n",
    "\n",
    "        display('(A) Possible feature values:', features)                                  # print results\n",
    "        display('(B) Number of possible combinations between all categorical data: {}'.format(func.reduce(operator.mul, features[val_count_col])))\n",
    "        return self\n",
    "\n",
    "q5_a = Q5_a().run(Dataset.load_sampled())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "**Table (A)** shows all possible values of the 5 categorical values.\n",
    "\n",
    "**Calculation (B)** shows the possible combinations between all categorical features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   (b) Choose one categorical feature and two non-categorical features as input features. Create a data set with those features and the target columns (different frogs). Name this data set *NN_data*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q5_b:\n",
    "    \n",
    "    input_features = [Dataset.Cols.acessR, Dataset.Cols.typeR, Dataset.Cols.fishingR]                  # Input features choosen\n",
    "    target_features = [Dataset.Cols.green_frogs, Dataset.Cols.tree_frogs, Dataset.Cols.brown_frogs]    # Target features choosen\n",
    "\n",
    "    def run(self, df:pd.DataFrame):                                       # creating a dataset with all \n",
    "        all_features = Q5_b.input_features.copy()                        # the required features and targets\n",
    "        for x in Q5_b.target_features: all_features.append(x)\n",
    "        self.NN_data = df[all_features]\n",
    "        return self\n",
    "\n",
    "q5_b = Q5_b().run(Dataset.load_sampled())\n",
    "NN_data = q5_b.NN_data                                   # taking the final attribute of instance and storing in the variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   (c) Convert the features that need to be converted using One-Hot-Encoding. Explain why you need (not) to convert these features. Name the data set *NN_data_encoded*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "* Converting all categorical features with one hot encoding.\n",
    "* All numeric features will be scaled for better learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q5_c:\n",
    "\n",
    "    def run(self, df:pd.DataFrame):\n",
    "        self.NN_data_encoded = pd.get_dummies(df)                        # to perform one-hot encoding\n",
    "        for (name, vals) in self.NN_data_encoded.iteritems():\n",
    "            if name in Q5_b.input_features and pd.api.types.is_numeric_dtype(vals.dtype):\n",
    "                self.NN_data_encoded[name] = StandardScaler().fit_transform(vals.values.reshape(-1,1))\n",
    "        self.encoded_inputs = [v for v in self.NN_data_encoded.columns.values if v not in Q5_b.target_features]\n",
    "        return self\n",
    "\n",
    "q5_c = Q5_c().run(NN_data)\n",
    "NN_data_encoded = q5_c.NN_data_encoded\n",
    "display(NN_data_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   (d) Create a training and test set with 90% of the rows of your *NN_data_encoded* data set for training and 10% as test data set. Name them *train_NN* and *test_NN*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q5_d:\n",
    "\n",
    "    def run(self, df):\n",
    "        self.train_NN = df.sample(frac=0.9, random_state = rand_seed)      # dividing the whole dataset into\n",
    "                                                                          # training and testing parts\n",
    "            \n",
    "        self.test_NN = df.drop(self.train_NN.index)                        # remove the taining part to obtain test part\n",
    "        return self\n",
    "\n",
    "q5_d = Q5_d().run(NN_data_encoded)\n",
    "train_NN = q5_d.train_NN\n",
    "test_NN = q5_d.test_NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   (e) Train two different Neural Networks, one with a linear activation function and one with a non-linear activation function. All other settings stay default. Give the accuracy of each Neural Network for the training and test set (*train_NN* and *test_NN*. Which activation function seems to be better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q5_e:\n",
    "    \n",
    "    activation_functions = [(\"identity\", \"linear\"),              # activation functions\n",
    "                            (\"logistic\", \"non-linear\"), \n",
    "                            (\"tanh\", \"non-linear\"), \n",
    "                            (\"relu\", \"non-linear\")]\n",
    "    \n",
    "    def run(self, train, test, inp, target):\n",
    "        # training each activation function and storing all models\n",
    "        models = [Q5_e.train_model(train, inp, target, activ_func[0]) for activ_func in Q5_e.activation_functions]\n",
    "        self.models = models\n",
    "        \n",
    "        # obtaining the prediction of the neural network for test set of each model \n",
    "        self.predicts = [model.predict(test[inp]) for model in models]\n",
    "        \n",
    "        # obtaing scores for each model\n",
    "        acc_scores_test = [MultiClassAccs(test[target], prediction) for model, prediction in zip(models, self.predicts)]\n",
    "        acc_scores_train = [MultiClassAccs(train[target], model.predict(train[inp])) for model in models]\n",
    "        self.scores = pd.DataFrame({'activation function':[a[0] for a in Q5_e.activation_functions], 'function type':[a[1] for a in Q5_e.activation_functions], 'score on test data':acc_scores_test, 'score on training data':acc_scores_train}).set_index('activation function')\n",
    "        return self\n",
    "     \n",
    "    # training neural network     \n",
    "    def train_model(train, inp, target, activ, **params):\n",
    "        model = MLPClassifier(random_state=rand_seed, activation=activ, **params)\n",
    "        model.fit(train[inp], train[target])\n",
    "        return model\n",
    "\n",
    "q5_e = Q5_e().run(train_NN, test_NN, q5_c.encoded_inputs, Q5_b.target_features)\n",
    "q5_e.scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: RELU and tanh activation functions performed best on the test data, with exactly the same scores. However, tanh performed slightly worse on the training data, which may be an indicator of a reduced tendency to overfit compared to the relu function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   (f) Based on your result of (e) train 2 more Neural Networks with different settings (change at least 4 parameters (2 each)). Explain your parameters and the choice of the activation function. Evaluate the different Neural Networks with your test set by giving the accuracy. Try to increase the accuracy and analyse the factors that prohibit better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: this class is used to select the best structure of the neural network by running many \n",
    "# possibilities (combinations). After obtaining the best neural network structure, we use it in further sections, \n",
    "# no need run each time\n",
    "\n",
    "class BruteForce_HL:\n",
    "\n",
    "    def __init__(self, train, test, inp, target, activation, lri):\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        self.inp = inp\n",
    "        self.target = target\n",
    "        self.activation = activation\n",
    "        self.lri = lri\n",
    "\n",
    "    def run(self, mn, mx, depth, best_acc= 0, top_layers=tuple()):\n",
    "        best_layers =  None\n",
    "        acc = lambda x: MultiClassAccs(self.test[self.target], x.predict(self.test[self.inp]))\n",
    "        for ln in range(mn,mx):\n",
    "            layers = top_layers + (ln,)\n",
    "            model = Q5_e.train_model(self.train, self.inp, self.target, self.activation, learning_rate_init=self.lri , hidden_layer_sizes=layers)\n",
    "            t_acc = acc(model) \n",
    "            if t_acc > best_acc or (t_acc == best_acc and best_layers and (len(layers) < len(best_layers) or (len(layers) == len(best_layers) and sum(layers) < sum(best_layers)))):\n",
    "                best_acc, best_layers = t_acc, layers\n",
    "                print(best_acc, best_layers)\n",
    "            if depth > 1:\n",
    "                o_acc, o_tuple = self.run(mn,mx,depth-1, best_acc, layers)\n",
    "                if o_tuple and (o_acc > best_acc or (o_acc == best_acc and best_layers and (len(o_tuple) < len(best_layers) or (len(o_tuple) == len(best_layers) and sum(o_tuple) < sum(best_layers))))):\n",
    "                    best_acc, best_layers = o_acc, o_tuple\n",
    "                    print(best_acc, best_layers)\n",
    "        return best_acc, best_layers\n",
    "\n",
    "#BruteForce_HL(train_NN, test_NN, q5_c.encoded_inputs, Q5_b.target_features).run(15,25,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BruteForce_HL(train_NN, test_NN, q5_c.encoded_inputs, Q5_b.target_features, \"tanh\", 0.001).run(15,25,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BruteForce_HL(train_NN, test_NN, q5_c.encoded_inputs, Q5_b.target_features, \"tanh\", 0.0005).run(15,25,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BruteForce_HL(train_NN, test_NN, q5_c.encoded_inputs, Q5_b.target_features, \"tanh\", 0.002).run(15,18,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: this class is used to select the best values of the neural network by running many possibilities (combinations). \n",
    "# no need run each time\n",
    "\n",
    "class BruteForce_Numeric:\n",
    "\n",
    "    def __init__(self, train, test, inp, target, activation):\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        self.inp = inp\n",
    "        self.target = target\n",
    "        self.activation = activation\n",
    "    \n",
    "    def run(self, param_name,rng ):\n",
    "        best_val =  None\n",
    "        best_acc = 0\n",
    "        acc = lambda x: MultiClassAccs(self.test[self.target], x.predict(self.test[self.inp]))\n",
    "        for param_v in rng:\n",
    "            model = Q5_e.train_model(self.train, self.inp, self.target, self.activation,hidden_layer_sizes=(20,24,16), **{param_name:param_v})\n",
    "            new_acc = acc(model)\n",
    "            if new_acc > best_acc:\n",
    "                best_acc = new_acc\n",
    "                best_val = param_v\n",
    "\n",
    "        return best_val, best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note:  this class is used to select the best parameters of the neural network by running many possibilities (combinations). \n",
    "# no need run each time\n",
    "\n",
    "import decimal\n",
    "\n",
    "def float_range(start, stop, step):\n",
    "  while start < stop:\n",
    "    yield start\n",
    "    start += step\n",
    "\n",
    "class BruteForce_Params:\n",
    "    params = [('solver', ['lbfgs', 'sgd', 'adam']), ('learning_rate', ['constant','invscaling','adaptive']), ('learning_rate_init', float_range(0.00001, 0.001, 0.00001))]\n",
    "\n",
    "    def __init__(self, train, test, inp, target):\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        self.inp = inp\n",
    "        self.target = target\n",
    "\n",
    "    def run(self, activ_functs):\n",
    "        for activ in activ_functs:\n",
    "            print(\"#####################\")\n",
    "            print(activ)\n",
    "            runner = BruteForce_Numeric(self.train, self.test, self.inp, self.target, activ)\n",
    "            for p_name, p_rng in BruteForce_Params.params:\n",
    "                print(\"Best value for {} is <<{}>> with an accuracy of {}\".format(p_name, *runner.run(p_name, p_rng)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BruteForce_Params(train_NN, test_NN, q5_c.encoded_inputs, Q5_b.target_features).run(['relu', 'tanh'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q5_f:\n",
    "\n",
    "    def run(self, train, test, inp, target):\n",
    "        # final models with choosen parameters, structure, and values are trained \n",
    "        self.model1 = Q5_e.train_model(train, inp, target, \"tanh\", learning_rate_init=0.0005, hidden_layer_sizes=(16,15,22))\n",
    "        self.model2 = Q5_e.train_model(train, inp, target, \"tanh\", learning_rate_init=0.002, hidden_layer_sizes=(15, 15, 16, 16, 15, 15, 17, 17))\n",
    "        \n",
    "        # predictions of the trained model is obtained\n",
    "        self.model1_pred = self.model1.predict(test[inp])\n",
    "        self.model2_pred = self.model2.predict(test[inp])\n",
    "        \n",
    "        # scores of various target features are printed\n",
    "        self.true_vals = test[target]\n",
    "        for name, predicted_vals in [(\"Model 1\", self.model1_pred), (\"Model 2\", self.model2_pred)]:\n",
    "            print(\"{} combination acc: {}\".format(name, metrics.accuracy_score(test[target], predicted_vals)))\n",
    "            for i, (col,vals) in enumerate(test[target].iteritems()):\n",
    "                print(\"\\t{} acc: {}\".format(col, metrics.accuracy_score(vals, [a[i] for a in predicted_vals])))\n",
    "\n",
    "        return self\n",
    "\n",
    "q5_f = Q5_f().run(train_NN, test_NN, q5_c.encoded_inputs, Q5_b.target_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification report for both models\n",
    "\n",
    "for pred in (q5_f.model1_pred, q5_f.model2_pred):\n",
    "    print(metrics.classification_report(q5_f.true_vals, pred, target_names=['Green', 'Tree', 'Brown']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 4 different structures of neural networks and activation functions,   ('relu',(20,24,16)),\n",
    "#                                                                           ('relu',(100,)), \n",
    "#                                                                           ('tanh',(15,18,22)),  \n",
    "#                                                                           ('tanh',(100,))\n",
    "# acccuracy of the models (y axis) are plotted against the learning rate (x axis)\n",
    "\n",
    "class Q5_visualize_score_steps:\n",
    "\n",
    "    def __init__(self, train, test, inp, target):\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        self.inp = inp\n",
    "        self.target = target\n",
    "\n",
    "    \n",
    "    def run(self, lfs):\n",
    "        for activ, layers in lfs:\n",
    "            self.run_model(activ,activ,layers)\n",
    "\n",
    "    def run_model(self, name, activ_func, layers):\n",
    "        scores = list()\n",
    "        acc = lambda x: MultiClassAccs(self.test[self.target], x.predict(self.test[self.inp]))\n",
    "        for v in float_range(0.001, 0.1, 0.001):\n",
    "            m = Q5_e.train_model(self.train, self.inp, self.target, activ_func, learning_rate_init=v, hidden_layer_sizes=layers)\n",
    "            a = acc(m)\n",
    "            scores.append(tuple((v,a)))\n",
    "            \n",
    "        df = pd.DataFrame({'learning_rate_init':[t[0] for t in scores], \n",
    "                           'combined':[t[1].total_combination_acc for t in scores], \n",
    "                           'green frogs':[t[1].individual_accs[Dataset.Cols.green_frogs] for t in scores], \n",
    "                           'tree frogs':[t[1].individual_accs[Dataset.Cols.tree_frogs] for t in scores], \n",
    "                           'brown frogs':[t[1].individual_accs[Dataset.Cols.brown_frogs] for t in scores]})\n",
    "        \n",
    "        df.plot(kind='line', x='learning_rate_init', y=['combined', 'green frogs', 'tree frogs', 'brown frogs'])\n",
    "\n",
    "_ = Q5_visualize_score_steps(train_NN, test_NN, q5_c.encoded_inputs, Q5_b.target_features).run([('relu',(20,24,16)),\n",
    "                                                                                                ('relu',(100,)), \n",
    "                                                                                                ('tanh',(15,18,22)), \n",
    "                                                                                                ('tanh',(100,))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: We tried to optimize various parameters to get a high acc with tanh function, as it looked like it would perform similiar to the relu function, but with a reduced tendency to overfit the data. We set various parameters and then determined the best number of hidden layers for accuracy. Halving the initial learning rate provided an additional 6% accuracy on green frogs, while not impacting any of the other classes. We could have propably scored an even higher acc with a) more hidden layers and their size and b) searching through parameter values programmaticly in combination with the hidden layers. However, this would cause the search space to explode. However, it is noteable that the performance of the model was mostly impacted by the number of hidden layers, rather then the size of those."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6 - Evaluation (10 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   (a) Consider two models of question 5 of your choice with the respective datasets (training and test data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create training and test data for 3-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating training and test data for both models\n",
    "\n",
    "class Q6:\n",
    "    X = NN_data_encoded[q5_c.encoded_inputs]\n",
    "    y = NN_data_encoded[Q5_b.target_features]\n",
    "    tts = TTSGroup(train_NN[q5_c.encoded_inputs], test_NN[q5_c.encoded_inputs], train_NN[Q5_b.target_features], test_NN[Q5_b.target_features])\n",
    "    models = [(\"Model 1\",q5_f.model1), (\"Model 2\", q5_f.model2)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Print confusion matrices on the training data and the cell-by-cell summation of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing confusion matrices\n",
    "\n",
    "for name, model in Q6.models:\n",
    "    print('###############')\n",
    "    print('{}:'.format(name))\n",
    "    pred = model.predict(Q6.tts.X.train)\n",
    "    all_conf = list()\n",
    "    for i, (target, vals) in enumerate(Q6.tts.y.train.iteritems()):\n",
    "        matrix = metrics.confusion_matrix(vals, [x[i] for x in pred])\n",
    "        print('Confusion Matrix for {}:\\n{}'.format(target, metrics.confusion_matrix(vals, [x[i] for x in pred])))\n",
    "        all_conf.append(matrix)\n",
    "    print('Total sum of all targets:\\n{}'.format(sum(all_conf)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Print the cell-by-cell summation of the confusion matrices on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell-by-cell summation of confusion matrices\n",
    "\n",
    "for name, model in Q6.models:\n",
    "    print('###############')\n",
    "    print('{}:'.format(name))\n",
    "    pred = model.predict(Q6.tts.X.test)\n",
    "    all_conf = list()\n",
    "    for i, (target, vals) in enumerate(Q6.tts.y.test.iteritems()):\n",
    "        matrix = metrics.confusion_matrix(vals, [x[i] for x in pred])\n",
    "        all_conf.append(matrix)\n",
    "    print('Total sum of all targets:\\n{}'.format(sum(all_conf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Print the precision, recall and f1-scores on the test data for each fold and model. Give the unaggregated results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing precision, recall, and f1-scores on the test data for each fold and model\n",
    "\n",
    "class Q6_kfold:\n",
    "    def run(self, models, X, y):\n",
    "        for mod_name, model in models:\n",
    "            print(\"###################################\\nModel {}\".format(mod_name))\n",
    "            kf = KFold(n_splits=3, random_state=rand_seed)                      # split the entire set into three fold data sets\n",
    "            for i, (train_index, test_index ) in enumerate(kf.split(X)):\n",
    "                X_train , X_test = X.iloc[train_index,:],X.iloc[test_index,:]\n",
    "                y_train , y_test = y.iloc[train_index] , y.iloc[test_index]\n",
    "                model.fit(X_train,y_train)\n",
    "                pred_values = model.predict(X_test)\n",
    "                print(\"----\\nFold {}\".format(i+1))                              # printing classification report foldwise\n",
    "                print(metrics.classification_report(y_test, pred_values, target_names=['Green', 'Tree', 'Brown']))\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "q6_kfold = Q6_kfold().run(Q6.models, Q6.X, Q6.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compute accuracy scores on training and test data (give explicitly the result for each fold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing accuracy scores on training data and test data for both models\n",
    "\n",
    "class Q6_accs:\n",
    "    def run(self, models, X, y):\n",
    "        for mod_name, model in models:\n",
    "            print(\"###################################\\nModel: {}\".format(mod_name))\n",
    "            kf = KFold(n_splits=3, random_state=rand_seed)                      # K fold split (K = 3)\n",
    "            train_accs = list()\n",
    "            test_accs = list()\n",
    "            for i, (train_index, test_index ) in enumerate(kf.split(X)):        # for giving result for each fold explicitly\n",
    "                X_train , X_test = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "                y_train , y_test = y.iloc[train_index] , y.iloc[test_index]\n",
    "                model.fit(X_train,y_train)\n",
    "                train_accs.append(MultiClassAccs(y_train, model.predict(X_train)))\n",
    "                test_accs.append(MultiClassAccs(y_test, model.predict(X_test)))\n",
    "            \n",
    "            Q6_accs.print_fold_accs(\"training\", train_accs)\n",
    "            Q6_accs.print_fold_accs(\"testing\", test_accs)\n",
    "        return self\n",
    "\n",
    "    def print_fold_accs(name, fold_accs):\n",
    "        print(\"---------\")\n",
    "        print(\"Accuracy on {} data:\".format(name))\n",
    "        for i, mc_acc in enumerate(fold_accs):\n",
    "            print(\"\\t Fold {}: {}\".format(i+1, mc_acc))\n",
    "        \n",
    "\n",
    "\n",
    "q6_accs = Q6_accs().run(Q6.models, Q6.X, Q6.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to turn numbers into insights, please comment on your findings. Motivate the answers to the following questions using the metrics and the findings in the **questions 2 through 5** of the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   (b) What is, in your opinion, the best model? Motivate your answer with the findings above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: The neural network, while having a good average score, seems to be to unreliable for this case. While trying to optimise one parameter would to get a good score on feature A but impacts at least one of the other target scores negatively. This is why we would choose multiple SVMs (one per target), as they can be optimised truely independently for the rather small provided dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   (c) Does any model suffer from underfitting or overfitting? Motivate your answer with the findings above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: As stated in question 2, tree2 suffers from overfitting. The second neural network is also more prone to overfitting, as we see in the second fold, where differences between training and test data accuracies are between 15% and 20%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7 - Clustering (8 points): "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) For this question, use the extracted data set you created in the preprocessing step (sampled_data). Drop all the columns expect \"VegetationR\", \"UseR\", \"FishingR\", \"RoadDistanceR\", \"BuildingR\", \"RoadDistanceR\", and \"PollutionR\". Use a dendogram to find the overview of the clusters that you can extract for the remaining columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping desired coulmns from sampled_data\n",
    "\n",
    "class Q7:\n",
    "    columns = [Dataset.Cols.vegetationR, Dataset.Cols.useR, Dataset.Cols.fishingR, Dataset.Cols.roadDistanceR, Dataset.Cols.buildingR, Dataset.Cols.pollutionR]\n",
    "    \n",
    "    def run(self, base_df):\n",
    "        self.df = base_df[Q7.columns]\n",
    "        return self\n",
    "\n",
    "q7 = Q7().run(Dataset.load_sampled())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q7_a:\n",
    "\n",
    "    def run(self, df):\n",
    "        normed = normalize(df)                                       # normalisation of data column-wise\n",
    "        self.scaled = pd.DataFrame(normed, columns=df.columns)\n",
    "        return self\n",
    "\n",
    "q7_a = Q7_a().run(q7.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dendogram to find the overview of the clusters \n",
    "\n",
    "plt.figure(figsize = (20,10))\n",
    "plt.title(\"Dendrogram\")\n",
    "plt.xlabel('instances / objects (row indices)')\n",
    "plt.ylabel('dissimilarity')\n",
    "_=shc.dendrogram(shc.linkage(q7_a.scaled, method = 'ward'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) What does the vertical and horizontal axis show in the dendogram? Why the distance between the clusters in the dendogram generally decreases, when we go from top to down in the dendogram?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Explaination: The vertical axis of this dendrogram represents the distance or dissimilarity between either either individual data points/clusters. The horizontal axis represents instances/objects (row indices).\n",
    " \n",
    "When we go from top to down in the dendogram, the distance between the clusters in the dendogram generally decreases since the dissimilarity between the clusters decreases i.e. similar clusters tend to be more close to each other than dissimilar ones. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Split the diagram at 3 and find the number the clusters at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting dendogram at 3\n",
    "\n",
    "plt.figure(figsize = (20,10))\n",
    "plt.title(\"Dendrogram\")\n",
    "plt.xlabel('instances / objects (row indices)')\n",
    "plt.ylabel('dissimilarity')\n",
    "plt.axhline(y = 3, color = 'b', linestyle = '--')\n",
    "_ = shc.dendrogram(shc.linkage(q7_a.scaled, method='ward'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explaination: In the figure above, the dendogram is splitted at 3 and three vertical lines can be observed at the split. \n",
    "Therefore, there are THREE clusters at this point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Using agglomerative clustering with the number of clusters found in the previous section and a scatter diagram, show the discovered cluster for \"VegetationR\" and \"UseR\" in different colors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agglomerative clustering with 3 clusters\n",
    "\n",
    "class Q7_d:\n",
    "\n",
    "    def run(self, df, xax, yax):\n",
    "        cluster = AgglomerativeClustering(n_clusters = 3, affinity = 'euclidean', linkage = 'ward')\n",
    "        cluster.fit_predict(df)\n",
    "\n",
    "        plt.figure(figsize = (15, 7))\n",
    "        colours = ListedColormap(['r','b','g'])         # colours for clusters\n",
    "        clusters = ['1', '2', '3']                      # for legend (shows cluster number)\n",
    "        scatter = plt.scatter(df['VegetationR'], df['UseR'], c = cluster.labels_, cmap = colours)    # scatter plot\n",
    "        plt.xlabel('VegetationR')\n",
    "        plt.ylabel('UseR')\n",
    "        plt.legend(handles = scatter.legend_elements()[0], labels = clusters) \n",
    "        plt.show()\n",
    "        return self\n",
    "\n",
    "q7_d = Q7_d().run(q7_a.scaled, Dataset.Cols.vegetationR, Dataset.Cols.useR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explaination: In the figure above, three clusters can be seen (red, blue, and green) for \"VegetationR\" and \"UseR\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
